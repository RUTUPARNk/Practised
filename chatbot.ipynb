{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4zXUMgZsbpnCVnJifvS2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RUTUPARNk/Practised/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Y_eUIo6jFC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "with open('intents.json') as file:\n",
        "  data = json.load(file)\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "for intent in data['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    training_sentences.append(pattern)\n",
        "    training_labels.append(intent['tag'])\n",
        "  responses.append(intent['responses'])\n",
        "\n",
        "  if intent['tag'] not in labels:\n",
        "    labels.append(intent['tag'])\n",
        "\n",
        "num_classes = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "training_labels = lbl_encoder.transform(training_labels)"
      ],
      "metadata": {
        "id": "N7dZHEQVII8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_len = 20\n",
        "oov_token = \"<00v>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen = max_len)\n"
      ],
      "metadata": {
        "id": "mYHljFzDIVWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "epochs = 500\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
      ],
      "metadata": {
        "id": "zfUHKUiLI0KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"chat_model\")\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "  pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
        "  pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "DmlVokfKXKth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import colorama\n",
        "colorama.init()\n",
        "from colorama import Fore, Style, Back\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "with open('Intents.json') as file:\n",
        "  data = json.load(file)\n",
        "def chat():\n",
        "  model = keras.models.load_model('chat_model')\n",
        "\n",
        "  with_open('tokenizer.pickle', 'rb') as handle:\n",
        "    lbl_encoder = pickle.load(enc)\n",
        "  max_len = 20\n",
        "\n",
        "  while True:\n",
        "    print(Fore.LIGHTBLUE_EX + \"User:\" + Style.RESET_ALL, end=\"\")\n",
        "    inp = input()\n",
        "    if inp.lower() == \"quit\":\n",
        "      break\n",
        "    result = model.predict(keras.preprocessing.sequence.pad_sequence(otkenizer.texts_truncating='post', maxlen=max_len))\n",
        "    tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
        "    for i in data['intents']:\n",
        "      if i['tag'] == tag:\n",
        "        print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL, np.random.choice(i['responses']))\n",
        "print(Fore.YELLOW + \"start messageing with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
        "chat()"
      ],
      "metadata": {
        "id": "le_64sTjXzFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}